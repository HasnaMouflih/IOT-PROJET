"""
Script pour r√©entra√Æner le mod√®le DIRECTEMENT avec les donn√©es de l'API Firebase
"""
import json
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
import xgboost as xgb
import joblib
import os
import requests

print(" R√©entra√Ænement du mod√®le avec donn√©es API (temps r√©el)...\n")

# ================================
# Chargement des donn√©es depuis l'API
# ================================
print(" Chargement des donn√©es depuis l'API...")
try:
    response = requests.get("http://localhost:5001/get_all_plants_list")
    api_data = response.json()
    
    if not api_data.get("success"):
        raise Exception("API request failed")
    
    # Extraire les readings
    all_readings = []
    for plant in api_data.get("plants", []):
        readings = plant.get("data", {})
        all_readings.extend(readings.values())
    
    print(f" {len(all_readings)} enregistrements charg√©s depuis l'API.\n")
except Exception as e:
    print(f" Erreur: {e}\n")
    exit(1)

if len(all_readings) < 20:
    print(f" ATTENTION: Seulement {len(all_readings)} enregistrements!")
    print("   Le mod√®le aura du mal √† apprendre avec si peu de donn√©es.\n")

# ================================
# Pr√©paration des donn√©es
# ================================
df = pd.DataFrame(all_readings)
print(f"DataFrame: {len(df)} lignes\n")

features = ["temperature", "humidity", "lightLevel", "soilMoisture"]
label_col = "emotion"

# V√©rifier qu'on a les colonnes n√©cessaires
missing_cols = [col for col in features + [label_col] if col not in df.columns]
if missing_cols:
    print(f" Colonnes manquantes: {missing_cols}")
    print(f"   Colonnes disponibles: {df.columns.tolist()}")
    exit(1)

# Encodage des √©motions
emotion_map = {v: i for i, v in enumerate(df[label_col].unique())}
df["emotion_encoded"] = df[label_col].map(emotion_map)

print(f" √âmotions d√©tect√©es: {emotion_map}")
print(f" Distribution:\n{df[label_col].value_counts()}\n")

# Normalisation
X_min, X_max = df[features].min(), df[features].max()
df_norm = (df[features] - X_min) / (X_max - X_min)

print(f" Min/Max d√©tect√©s:")
for feat in features:
    print(f"   {feat}: [{X_min[feat]:.2f}, {X_max[feat]:.2f}]")
print()

# ================================
#  Construction des s√©quences LSTM
# ================================
sequence_length = min(5, len(df) // 2)  # Adapt√© √† la taille des donn√©es
print(f" S√©quence LSTM: {sequence_length} enregistrements\n")

X_sequences, y_future = [], []

for i in range(len(df_norm) - sequence_length):
    seq = df_norm.iloc[i:i + sequence_length].values
    next_values = df_norm.iloc[i + sequence_length].values
    X_sequences.append(seq)
    y_future.append(next_values)

X_sequences, y_future = np.array(X_sequences), np.array(y_future)

if len(X_sequences) < 5:
    print(f"‚ö†Ô∏è ERREUR: Seulement {len(X_sequences)} s√©quences pour entra√Ænement!")
    print("   Minimum requis: 5 s√©quences")
    print("   Vous avez {0} enregistrements, besoin d'au moins {1}.\n".format(
        len(df), sequence_length * 3))
    print("üí° Solutions:")
    print("   1. Collectez plus de donn√©es (au moins 20-30 enregistrements)")
    print("   2. R√©duisez sequence_length")
    print("   3. V√©rifiez que l'API retourne des donn√©es\n")
    exit(1)

print(f"‚úÖ Donn√©es LSTM: {X_sequences.shape}")
print(f"‚úÖ Labels futurs: {y_future.shape}\n")

# ================================
#  Mod√®le LSTM
# ================================
print(" Construction du mod√®le LSTM...")
model_lstm = Sequential([
    Input(shape=(sequence_length, len(features))),
    LSTM(32, activation='relu', return_sequences=False),  # R√©duit pour peu de donn√©es
    Dense(len(features), activation='linear')
])

model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

print(" Entra√Ænement du mod√®le LSTM...")
model_lstm.fit(X_sequences, y_future, epochs=50, batch_size=4, verbose=0)
print("Entra√Ænement du LSTM termin√©.\n")

# ================================
# üîÑ Pr√©dictions pour XGBoost
# ================================
print(" G√©n√©ration des pr√©dictions LSTM...")
predicted_future = model_lstm.predict(X_sequences, verbose=0)
print(" Pr√©dictions g√©n√©r√©es.\n")

# ================================
#  Mod√®le XGBoost
# ================================
y_emotions = df["emotion_encoded"].iloc[sequence_length:].values
X_train, X_test, y_train, y_test = train_test_split(
    predicted_future, y_emotions, test_size=0.2, random_state=42
)

print(f" Donn√©es XGBoost:")
print(f"   Train: {X_train.shape}, Test: {X_test.shape}\n")

print("Entra√Ænement du mod√®le XGBoost...")
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=4,
    random_state=42
)
xgb_model.fit(X_train, y_train)
print("‚úÖ Entra√Ænement du XGBoost termin√©.\n")

# √âvaluation
train_score = xgb_model.score(X_train, y_train)
test_score = xgb_model.score(X_test, y_test)
print(f"Score XGBoost:")
print(f"   Train: {train_score:.3f}")
print(f"   Test: {test_score:.3f}\n")

# ================================
#  Sauvegarde
# ================================
base_path = os.path.dirname(__file__)

print(" Sauvegarde des mod√®les...")
model_lstm.save(os.path.join(base_path, "lstm_future_model.h5"))
joblib.dump(xgb_model, os.path.join(base_path, "xgboost_emotion_model.pkl"))

# Sauvegarde des param√®tres
scaling_info = {
    "X_min": X_min.to_dict(),
    "X_max": X_max.to_dict(),
    "emotion_map": emotion_map,
    "data_source": "API Firebase (temps r√©el)",
    "total_records": len(df),
    "sequence_length": sequence_length,
    "lstm_sequences": len(X_sequences),
    "xgb_train_score": float(train_score),
    "xgb_test_score": float(test_score)
}

with open(os.path.join(base_path, "scaling_info.json"), "w", encoding="utf-8") as f:
    json.dump(scaling_info, f, indent=4, ensure_ascii=False)

print(" Mod√®les sauvegard√©s:")
print("   - lstm_future_model.h5")
print("   - xgboost_emotion_model.pkl")
print("   - scaling_info.json\n")

print(" R√©sum√©:")
print(f"   Donn√©es utilis√©es: {len(df)} enregistrements de l'API")
print(f"   S√©quences LSTM: {len(X_sequences)}")
print(f"   √âmotions: {list(emotion_map.keys())}")
print(f"   Score XGBoost (test): {test_score:.1%}\n")

print(" R√©entra√Ænement termin√©!")
 
